{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_04_Using_Optimizers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aravinda89/Pytorch-Tutorial/blob/main/03_04_Using_Optimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBehvqZ8znsy"
      },
      "source": [
        "# Using optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCdIqY0tKbvS"
      },
      "source": [
        "# Setting seeds to try and ensure we have the same results - this is not guaranteed across PyTorch releases.\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCJzXv0OK1Bs"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "mean, std = (0.5,), (0.5,)\n",
        "\n",
        "# Create a transform and normalise data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean, std)\n",
        "                              ])\n",
        "\n",
        "# Download FMNIST training dataset and load training data\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download FMNIST test dataset and load test data\n",
        "testset = datasets.FashionMNIST('~/.pytorch/FMNIST/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZpZ12MrEDZI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqMqFbIVrbFH"
      },
      "source": [
        "class FMNIST(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(784, 128)\n",
        "    self.fc2 = nn.Linear(128,64)\n",
        "    self.fc3 = nn.Linear(64,10)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    \n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "    \n",
        "    return x\n",
        "    \n",
        "#model = FMNIST()   "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m68OeMRdEF0X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c0QgxCF3fD-"
      },
      "source": [
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 10),\n",
        "                      nn.LogSoftmax(dim=1))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjBut_7lhAc8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2ZAGFzFEQA_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iPQek2nz2yu"
      },
      "source": [
        "images, labels = next(iter(trainloader))\n",
        "images = images.view(images.shape[0], -1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMnVwV-CERd_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b2faed9-fdfa-4c11-e7c9-15c045d38568"
      },
      "source": [
        "model.parameters()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7f8dae89d550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roihp-kN0Jw5"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvbHIyPSEUPh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtP3nCEQEUMH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwcPkxQwEfYX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nf2WdmP5Gst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4571162d-ce25-44ce-b399-64fdb39d3874"
      },
      "source": [
        "output = model(images)\n",
        "loss = criterion(output, labels)\n",
        "loss.backward()\n",
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)\n",
        "        "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
            "        [-0.0198, -0.0150, -0.0104,  ..., -0.0203, -0.0060, -0.0299],\n",
            "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0295,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0233, -0.0220, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0066,  0.0125,  ...,  0.0286,  0.0350, -0.0105]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  tensor([[-7.2154e-04, -7.2154e-04, -7.2154e-04,  ..., -7.1856e-04,\n",
            "         -7.2154e-04, -7.2154e-04],\n",
            "        [ 1.5117e-03,  1.5168e-03,  1.5292e-03,  ...,  1.5526e-03,\n",
            "          1.5154e-03,  1.5168e-03],\n",
            "        [-4.5585e-04, -4.5585e-04, -4.5585e-04,  ..., -4.5585e-04,\n",
            "         -4.5585e-04, -4.5585e-04],\n",
            "        ...,\n",
            "        [-9.6583e-05, -9.6583e-05, -9.6583e-05,  ..., -1.5217e-04,\n",
            "         -9.6583e-05, -9.6583e-05],\n",
            "        [-5.6446e-04, -5.6559e-04, -5.5394e-04,  ..., -5.1035e-04,\n",
            "         -5.6446e-04, -5.6559e-04],\n",
            "        [ 3.7556e-03,  3.7518e-03,  3.7486e-03,  ...,  3.7677e-03,\n",
            "          3.7556e-03,  3.7518e-03]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arwzAK-1EkEH"
      },
      "source": [
        "optimizer.step()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD-u49yzEj6v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuGKi_nq6P0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b41cfcbe-2b92-406a-922c-c5f9e0ff0d2e"
      },
      "source": [
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
            "        [-0.0198, -0.0150, -0.0105,  ..., -0.0203, -0.0060, -0.0300],\n",
            "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0295,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0232, -0.0220, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0065,  0.0125,  ...,  0.0285,  0.0349, -0.0106]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  tensor([[-7.2154e-04, -7.2154e-04, -7.2154e-04,  ..., -7.1856e-04,\n",
            "         -7.2154e-04, -7.2154e-04],\n",
            "        [ 1.5117e-03,  1.5168e-03,  1.5292e-03,  ...,  1.5526e-03,\n",
            "          1.5154e-03,  1.5168e-03],\n",
            "        [-4.5585e-04, -4.5585e-04, -4.5585e-04,  ..., -4.5585e-04,\n",
            "         -4.5585e-04, -4.5585e-04],\n",
            "        ...,\n",
            "        [-9.6583e-05, -9.6583e-05, -9.6583e-05,  ..., -1.5217e-04,\n",
            "         -9.6583e-05, -9.6583e-05],\n",
            "        [-5.6446e-04, -5.6559e-04, -5.5394e-04,  ..., -5.1035e-04,\n",
            "         -5.6446e-04, -5.6559e-04],\n",
            "        [ 3.7556e-03,  3.7518e-03,  3.7486e-03,  ...,  3.7677e-03,\n",
            "          3.7556e-03,  3.7518e-03]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8oIy5SkEpDn"
      },
      "source": [
        "optimizer.zero_grad()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnfpzGigEpAr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EniqxHDwDa8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b69c6e6-1390-4d58-a6e7-53cf1d0f9c49"
      },
      "source": [
        "print('Initial weights : ',model[0].weight)\n",
        "print('Initial weights gradient : ',model[0].weight.grad)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial weights :  Parameter containing:\n",
            "tensor([[-0.0003,  0.0192, -0.0294,  ...,  0.0219,  0.0037,  0.0021],\n",
            "        [-0.0198, -0.0150, -0.0105,  ..., -0.0203, -0.0060, -0.0300],\n",
            "        [-0.0201,  0.0149, -0.0333,  ..., -0.0203,  0.0012,  0.0080],\n",
            "        ...,\n",
            "        [ 0.0018, -0.0295,  0.0085,  ..., -0.0037,  0.0036,  0.0300],\n",
            "        [-0.0232, -0.0220, -0.0064,  ...,  0.0115, -0.0324, -0.0158],\n",
            "        [ 0.0309,  0.0065,  0.0125,  ...,  0.0285,  0.0349, -0.0106]],\n",
            "       requires_grad=True)\n",
            "Initial weights gradient :  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DViAViGEwyr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGZhQE3tDcqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66308bea-220b-4234-b853-d847b8227cd0"
      },
      "source": [
        "model = FMNIST()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    cum_loss = 0\n",
        "    batch_num = 0\n",
        "\n",
        "    for batch_num, (images, labels) in enumerate(trainloader,1):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        cum_loss += loss.item()\n",
        "        print(f'Batch: {batch_num}, Loss: {loss.item()}')\n",
        "     \n",
        "    print(f\"Training loss: {cum_loss/len(trainloader)}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch: 1, Loss: 2.288628339767456\n",
            "Batch: 2, Loss: 2.3080620765686035\n",
            "Batch: 3, Loss: 2.3098013401031494\n",
            "Batch: 4, Loss: 2.284043312072754\n",
            "Batch: 5, Loss: 2.291738986968994\n",
            "Batch: 6, Loss: 2.3061680793762207\n",
            "Batch: 7, Loss: 2.2870423793792725\n",
            "Batch: 8, Loss: 2.276984453201294\n",
            "Batch: 9, Loss: 2.2776780128479004\n",
            "Batch: 10, Loss: 2.3024075031280518\n",
            "Batch: 11, Loss: 2.270155906677246\n",
            "Batch: 12, Loss: 2.288559913635254\n",
            "Batch: 13, Loss: 2.2820968627929688\n",
            "Batch: 14, Loss: 2.2649171352386475\n",
            "Batch: 15, Loss: 2.274874210357666\n",
            "Batch: 16, Loss: 2.2665631771087646\n",
            "Batch: 17, Loss: 2.2486894130706787\n",
            "Batch: 18, Loss: 2.25093150138855\n",
            "Batch: 19, Loss: 2.2826104164123535\n",
            "Batch: 20, Loss: 2.2532405853271484\n",
            "Batch: 21, Loss: 2.234309434890747\n",
            "Batch: 22, Loss: 2.2593002319335938\n",
            "Batch: 23, Loss: 2.248821496963501\n",
            "Batch: 24, Loss: 2.2512760162353516\n",
            "Batch: 25, Loss: 2.2572567462921143\n",
            "Batch: 26, Loss: 2.2401793003082275\n",
            "Batch: 27, Loss: 2.214656352996826\n",
            "Batch: 28, Loss: 2.226203441619873\n",
            "Batch: 29, Loss: 2.2151875495910645\n",
            "Batch: 30, Loss: 2.2176034450531006\n",
            "Batch: 31, Loss: 2.23728084564209\n",
            "Batch: 32, Loss: 2.2319467067718506\n",
            "Batch: 33, Loss: 2.2288172245025635\n",
            "Batch: 34, Loss: 2.2359671592712402\n",
            "Batch: 35, Loss: 2.2106549739837646\n",
            "Batch: 36, Loss: 2.2009663581848145\n",
            "Batch: 37, Loss: 2.228806495666504\n",
            "Batch: 38, Loss: 2.2145836353302\n",
            "Batch: 39, Loss: 2.2042295932769775\n",
            "Batch: 40, Loss: 2.1886141300201416\n",
            "Batch: 41, Loss: 2.20263409614563\n",
            "Batch: 42, Loss: 2.199158191680908\n",
            "Batch: 43, Loss: 2.169827938079834\n",
            "Batch: 44, Loss: 2.14996075630188\n",
            "Batch: 45, Loss: 2.1614444255828857\n",
            "Batch: 46, Loss: 2.1536142826080322\n",
            "Batch: 47, Loss: 2.1791305541992188\n",
            "Batch: 48, Loss: 2.174588680267334\n",
            "Batch: 49, Loss: 2.1909241676330566\n",
            "Batch: 50, Loss: 2.167595624923706\n",
            "Batch: 51, Loss: 2.170773506164551\n",
            "Batch: 52, Loss: 2.176833391189575\n",
            "Batch: 53, Loss: 2.1565167903900146\n",
            "Batch: 54, Loss: 2.119532823562622\n",
            "Batch: 55, Loss: 2.1230435371398926\n",
            "Batch: 56, Loss: 2.1416871547698975\n",
            "Batch: 57, Loss: 2.100715398788452\n",
            "Batch: 58, Loss: 2.1146626472473145\n",
            "Batch: 59, Loss: 2.1192877292633057\n",
            "Batch: 60, Loss: 2.143620729446411\n",
            "Batch: 61, Loss: 2.120922565460205\n",
            "Batch: 62, Loss: 2.0875020027160645\n",
            "Batch: 63, Loss: 2.087580442428589\n",
            "Batch: 64, Loss: 2.104800224304199\n",
            "Batch: 65, Loss: 2.1346566677093506\n",
            "Batch: 66, Loss: 2.105095624923706\n",
            "Batch: 67, Loss: 2.0743093490600586\n",
            "Batch: 68, Loss: 2.1175568103790283\n",
            "Batch: 69, Loss: 2.0749971866607666\n",
            "Batch: 70, Loss: 2.040395498275757\n",
            "Batch: 71, Loss: 2.0589356422424316\n",
            "Batch: 72, Loss: 2.0627200603485107\n",
            "Batch: 73, Loss: 2.062371015548706\n",
            "Batch: 74, Loss: 2.0711536407470703\n",
            "Batch: 75, Loss: 2.0451242923736572\n",
            "Batch: 76, Loss: 1.9929347038269043\n",
            "Batch: 77, Loss: 2.093440055847168\n",
            "Batch: 78, Loss: 2.0415003299713135\n",
            "Batch: 79, Loss: 1.9999783039093018\n",
            "Batch: 80, Loss: 2.0731618404388428\n",
            "Batch: 81, Loss: 2.0099408626556396\n",
            "Batch: 82, Loss: 1.966227650642395\n",
            "Batch: 83, Loss: 1.996374249458313\n",
            "Batch: 84, Loss: 1.9500383138656616\n",
            "Batch: 85, Loss: 2.0324738025665283\n",
            "Batch: 86, Loss: 2.0362255573272705\n",
            "Batch: 87, Loss: 1.9684059619903564\n",
            "Batch: 88, Loss: 1.947343111038208\n",
            "Batch: 89, Loss: 2.020293951034546\n",
            "Batch: 90, Loss: 1.9370741844177246\n",
            "Batch: 91, Loss: 2.0110342502593994\n",
            "Batch: 92, Loss: 1.973934531211853\n",
            "Batch: 93, Loss: 1.9326590299606323\n",
            "Batch: 94, Loss: 1.9520378112792969\n",
            "Batch: 95, Loss: 1.948966383934021\n",
            "Batch: 96, Loss: 1.9084886312484741\n",
            "Batch: 97, Loss: 1.970387578010559\n",
            "Batch: 98, Loss: 1.9823907613754272\n",
            "Batch: 99, Loss: 1.9808343648910522\n",
            "Batch: 100, Loss: 2.0078608989715576\n",
            "Batch: 101, Loss: 1.9658935070037842\n",
            "Batch: 102, Loss: 1.9703660011291504\n",
            "Batch: 103, Loss: 1.9045119285583496\n",
            "Batch: 104, Loss: 1.9508435726165771\n",
            "Batch: 105, Loss: 1.9133565425872803\n",
            "Batch: 106, Loss: 1.8932511806488037\n",
            "Batch: 107, Loss: 1.8733527660369873\n",
            "Batch: 108, Loss: 1.8558356761932373\n",
            "Batch: 109, Loss: 1.8342621326446533\n",
            "Batch: 110, Loss: 1.8201647996902466\n",
            "Batch: 111, Loss: 1.883374571800232\n",
            "Batch: 112, Loss: 1.817347764968872\n",
            "Batch: 113, Loss: 1.8530877828598022\n",
            "Batch: 114, Loss: 1.7657442092895508\n",
            "Batch: 115, Loss: 1.8477519750595093\n",
            "Batch: 116, Loss: 1.71091628074646\n",
            "Batch: 117, Loss: 1.9003044366836548\n",
            "Batch: 118, Loss: 1.7871630191802979\n",
            "Batch: 119, Loss: 1.8380069732666016\n",
            "Batch: 120, Loss: 1.7534091472625732\n",
            "Batch: 121, Loss: 1.9201205968856812\n",
            "Batch: 122, Loss: 1.8262723684310913\n",
            "Batch: 123, Loss: 1.7914384603500366\n",
            "Batch: 124, Loss: 1.7844767570495605\n",
            "Batch: 125, Loss: 1.811193585395813\n",
            "Batch: 126, Loss: 1.7456810474395752\n",
            "Batch: 127, Loss: 1.7881203889846802\n",
            "Batch: 128, Loss: 1.7524603605270386\n",
            "Batch: 129, Loss: 1.8272675275802612\n",
            "Batch: 130, Loss: 1.7048660516738892\n",
            "Batch: 131, Loss: 1.677865982055664\n",
            "Batch: 132, Loss: 1.714766263961792\n",
            "Batch: 133, Loss: 1.722551941871643\n",
            "Batch: 134, Loss: 1.6499258279800415\n",
            "Batch: 135, Loss: 1.662566065788269\n",
            "Batch: 136, Loss: 1.7684403657913208\n",
            "Batch: 137, Loss: 1.7792322635650635\n",
            "Batch: 138, Loss: 1.6747993230819702\n",
            "Batch: 139, Loss: 1.6495933532714844\n",
            "Batch: 140, Loss: 1.6461374759674072\n",
            "Batch: 141, Loss: 1.7905811071395874\n",
            "Batch: 142, Loss: 1.6514962911605835\n",
            "Batch: 143, Loss: 1.6357617378234863\n",
            "Batch: 144, Loss: 1.6526944637298584\n",
            "Batch: 145, Loss: 1.6028043031692505\n",
            "Batch: 146, Loss: 1.5982978343963623\n",
            "Batch: 147, Loss: 1.6614681482315063\n",
            "Batch: 148, Loss: 1.633899450302124\n",
            "Batch: 149, Loss: 1.6505759954452515\n",
            "Batch: 150, Loss: 1.6519851684570312\n",
            "Batch: 151, Loss: 1.5536835193634033\n",
            "Batch: 152, Loss: 1.6731449365615845\n",
            "Batch: 153, Loss: 1.5891188383102417\n",
            "Batch: 154, Loss: 1.6247167587280273\n",
            "Batch: 155, Loss: 1.6288293600082397\n",
            "Batch: 156, Loss: 1.54936945438385\n",
            "Batch: 157, Loss: 1.6293458938598633\n",
            "Batch: 158, Loss: 1.6461557149887085\n",
            "Batch: 159, Loss: 1.7075822353363037\n",
            "Batch: 160, Loss: 1.5601856708526611\n",
            "Batch: 161, Loss: 1.6359786987304688\n",
            "Batch: 162, Loss: 1.6543328762054443\n",
            "Batch: 163, Loss: 1.589802622795105\n",
            "Batch: 164, Loss: 1.5378397703170776\n",
            "Batch: 165, Loss: 1.5736500024795532\n",
            "Batch: 166, Loss: 1.5079909563064575\n",
            "Batch: 167, Loss: 1.442003607749939\n",
            "Batch: 168, Loss: 1.5722631216049194\n",
            "Batch: 169, Loss: 1.6032257080078125\n",
            "Batch: 170, Loss: 1.5143022537231445\n",
            "Batch: 171, Loss: 1.5212253332138062\n",
            "Batch: 172, Loss: 1.4533815383911133\n",
            "Batch: 173, Loss: 1.6026699542999268\n",
            "Batch: 174, Loss: 1.4409538507461548\n",
            "Batch: 175, Loss: 1.4144580364227295\n",
            "Batch: 176, Loss: 1.4812142848968506\n",
            "Batch: 177, Loss: 1.5983364582061768\n",
            "Batch: 178, Loss: 1.3538991212844849\n",
            "Batch: 179, Loss: 1.4719982147216797\n",
            "Batch: 180, Loss: 1.3759145736694336\n",
            "Batch: 181, Loss: 1.5867687463760376\n",
            "Batch: 182, Loss: 1.5142546892166138\n",
            "Batch: 183, Loss: 1.4200968742370605\n",
            "Batch: 184, Loss: 1.3368122577667236\n",
            "Batch: 185, Loss: 1.4060966968536377\n",
            "Batch: 186, Loss: 1.3488173484802246\n",
            "Batch: 187, Loss: 1.3889533281326294\n",
            "Batch: 188, Loss: 1.444939136505127\n",
            "Batch: 189, Loss: 1.4007610082626343\n",
            "Batch: 190, Loss: 1.3080072402954102\n",
            "Batch: 191, Loss: 1.4354183673858643\n",
            "Batch: 192, Loss: 1.412835717201233\n",
            "Batch: 193, Loss: 1.4986944198608398\n",
            "Batch: 194, Loss: 1.4947781562805176\n",
            "Batch: 195, Loss: 1.3933794498443604\n",
            "Batch: 196, Loss: 1.3159403800964355\n",
            "Batch: 197, Loss: 1.2502892017364502\n",
            "Batch: 198, Loss: 1.4704383611679077\n",
            "Batch: 199, Loss: 1.339613437652588\n",
            "Batch: 200, Loss: 1.3669652938842773\n",
            "Batch: 201, Loss: 1.4082685708999634\n",
            "Batch: 202, Loss: 1.3766931295394897\n",
            "Batch: 203, Loss: 1.3584290742874146\n",
            "Batch: 204, Loss: 1.4084861278533936\n",
            "Batch: 205, Loss: 1.3592095375061035\n",
            "Batch: 206, Loss: 1.3072935342788696\n",
            "Batch: 207, Loss: 1.3820505142211914\n",
            "Batch: 208, Loss: 1.3444007635116577\n",
            "Batch: 209, Loss: 1.2358068227767944\n",
            "Batch: 210, Loss: 1.3160265684127808\n",
            "Batch: 211, Loss: 1.2498250007629395\n",
            "Batch: 212, Loss: 1.4003181457519531\n",
            "Batch: 213, Loss: 1.2763175964355469\n",
            "Batch: 214, Loss: 1.3032809495925903\n",
            "Batch: 215, Loss: 1.1196180582046509\n",
            "Batch: 216, Loss: 1.342041015625\n",
            "Batch: 217, Loss: 1.320095181465149\n",
            "Batch: 218, Loss: 1.4522000551223755\n",
            "Batch: 219, Loss: 1.3721051216125488\n",
            "Batch: 220, Loss: 1.2343941926956177\n",
            "Batch: 221, Loss: 1.309903860092163\n",
            "Batch: 222, Loss: 1.2885193824768066\n",
            "Batch: 223, Loss: 1.267830491065979\n",
            "Batch: 224, Loss: 1.2900925874710083\n",
            "Batch: 225, Loss: 1.2122211456298828\n",
            "Batch: 226, Loss: 1.2932581901550293\n",
            "Batch: 227, Loss: 1.2226699590682983\n",
            "Batch: 228, Loss: 1.281243920326233\n",
            "Batch: 229, Loss: 1.3463259935379028\n",
            "Batch: 230, Loss: 1.2443780899047852\n",
            "Batch: 231, Loss: 1.2140682935714722\n",
            "Batch: 232, Loss: 1.3427790403366089\n",
            "Batch: 233, Loss: 1.115907907485962\n",
            "Batch: 234, Loss: 1.2040200233459473\n",
            "Batch: 235, Loss: 1.4018675088882446\n",
            "Batch: 236, Loss: 1.3054977655410767\n",
            "Batch: 237, Loss: 1.1856942176818848\n",
            "Batch: 238, Loss: 1.2555208206176758\n",
            "Batch: 239, Loss: 1.1556649208068848\n",
            "Batch: 240, Loss: 1.1539603471755981\n",
            "Batch: 241, Loss: 1.1964763402938843\n",
            "Batch: 242, Loss: 1.1661659479141235\n",
            "Batch: 243, Loss: 1.3382188081741333\n",
            "Batch: 244, Loss: 1.0965142250061035\n",
            "Batch: 245, Loss: 1.173755168914795\n",
            "Batch: 246, Loss: 1.2829060554504395\n",
            "Batch: 247, Loss: 1.1114954948425293\n",
            "Batch: 248, Loss: 1.2254295349121094\n",
            "Batch: 249, Loss: 1.1858655214309692\n",
            "Batch: 250, Loss: 1.1011098623275757\n",
            "Batch: 251, Loss: 1.23004949092865\n",
            "Batch: 252, Loss: 1.1434879302978516\n",
            "Batch: 253, Loss: 1.0658663511276245\n",
            "Batch: 254, Loss: 1.0187327861785889\n",
            "Batch: 255, Loss: 1.2080317735671997\n",
            "Batch: 256, Loss: 1.1579567193984985\n",
            "Batch: 257, Loss: 1.226787805557251\n",
            "Batch: 258, Loss: 1.1264475584030151\n",
            "Batch: 259, Loss: 1.093930721282959\n",
            "Batch: 260, Loss: 1.1958645582199097\n",
            "Batch: 261, Loss: 1.254819393157959\n",
            "Batch: 262, Loss: 1.143178939819336\n",
            "Batch: 263, Loss: 1.0960232019424438\n",
            "Batch: 264, Loss: 1.0613385438919067\n",
            "Batch: 265, Loss: 1.1455409526824951\n",
            "Batch: 266, Loss: 1.1083053350448608\n",
            "Batch: 267, Loss: 1.1469074487686157\n",
            "Batch: 268, Loss: 1.0699877738952637\n",
            "Batch: 269, Loss: 1.1545144319534302\n",
            "Batch: 270, Loss: 1.1117990016937256\n",
            "Batch: 271, Loss: 1.106830358505249\n",
            "Batch: 272, Loss: 1.0274953842163086\n",
            "Batch: 273, Loss: 1.0516313314437866\n",
            "Batch: 274, Loss: 1.106871247291565\n",
            "Batch: 275, Loss: 0.9517826437950134\n",
            "Batch: 276, Loss: 1.097378134727478\n",
            "Batch: 277, Loss: 1.0850956439971924\n",
            "Batch: 278, Loss: 1.0541390180587769\n",
            "Batch: 279, Loss: 1.050610899925232\n",
            "Batch: 280, Loss: 1.221909523010254\n",
            "Batch: 281, Loss: 1.0500787496566772\n",
            "Batch: 282, Loss: 1.1538501977920532\n",
            "Batch: 283, Loss: 1.003629446029663\n",
            "Batch: 284, Loss: 1.0411653518676758\n",
            "Batch: 285, Loss: 1.0567150115966797\n",
            "Batch: 286, Loss: 1.1080623865127563\n",
            "Batch: 287, Loss: 0.9144642353057861\n",
            "Batch: 288, Loss: 1.1202350854873657\n",
            "Batch: 289, Loss: 1.110558032989502\n",
            "Batch: 290, Loss: 1.1209615468978882\n",
            "Batch: 291, Loss: 1.090047836303711\n",
            "Batch: 292, Loss: 0.963047206401825\n",
            "Batch: 293, Loss: 1.0489884614944458\n",
            "Batch: 294, Loss: 1.0792567729949951\n",
            "Batch: 295, Loss: 1.0308566093444824\n",
            "Batch: 296, Loss: 1.0164964199066162\n",
            "Batch: 297, Loss: 1.024631142616272\n",
            "Batch: 298, Loss: 1.0618395805358887\n",
            "Batch: 299, Loss: 0.971246600151062\n",
            "Batch: 300, Loss: 1.1577436923980713\n",
            "Batch: 301, Loss: 0.9831182956695557\n",
            "Batch: 302, Loss: 0.9848618507385254\n",
            "Batch: 303, Loss: 0.9847303032875061\n",
            "Batch: 304, Loss: 0.8740224242210388\n",
            "Batch: 305, Loss: 0.9627154469490051\n",
            "Batch: 306, Loss: 1.0101865530014038\n",
            "Batch: 307, Loss: 1.0947006940841675\n",
            "Batch: 308, Loss: 0.963843047618866\n",
            "Batch: 309, Loss: 0.9141923189163208\n",
            "Batch: 310, Loss: 1.0287573337554932\n",
            "Batch: 311, Loss: 1.1636847257614136\n",
            "Batch: 312, Loss: 0.8538345098495483\n",
            "Batch: 313, Loss: 0.9889305830001831\n",
            "Batch: 314, Loss: 1.0358108282089233\n",
            "Batch: 315, Loss: 0.9372117519378662\n",
            "Batch: 316, Loss: 0.9494582414627075\n",
            "Batch: 317, Loss: 1.0457823276519775\n",
            "Batch: 318, Loss: 0.8595944046974182\n",
            "Batch: 319, Loss: 1.0612674951553345\n",
            "Batch: 320, Loss: 1.0196110010147095\n",
            "Batch: 321, Loss: 0.8770537376403809\n",
            "Batch: 322, Loss: 0.9593400955200195\n",
            "Batch: 323, Loss: 0.9212438464164734\n",
            "Batch: 324, Loss: 0.8432738780975342\n",
            "Batch: 325, Loss: 0.896544873714447\n",
            "Batch: 326, Loss: 0.9513491988182068\n",
            "Batch: 327, Loss: 1.0094587802886963\n",
            "Batch: 328, Loss: 0.9549076557159424\n",
            "Batch: 329, Loss: 0.8125507831573486\n",
            "Batch: 330, Loss: 0.9265959858894348\n",
            "Batch: 331, Loss: 1.0262107849121094\n",
            "Batch: 332, Loss: 1.0241512060165405\n",
            "Batch: 333, Loss: 0.9229015707969666\n",
            "Batch: 334, Loss: 1.019247055053711\n",
            "Batch: 335, Loss: 0.9702876806259155\n",
            "Batch: 336, Loss: 0.8675971031188965\n",
            "Batch: 337, Loss: 0.8122143745422363\n",
            "Batch: 338, Loss: 0.995839536190033\n",
            "Batch: 339, Loss: 0.8138614892959595\n",
            "Batch: 340, Loss: 0.9475014805793762\n",
            "Batch: 341, Loss: 0.9903124570846558\n",
            "Batch: 342, Loss: 0.9694657921791077\n",
            "Batch: 343, Loss: 0.8020482063293457\n",
            "Batch: 344, Loss: 0.8098590970039368\n",
            "Batch: 345, Loss: 0.8375106453895569\n",
            "Batch: 346, Loss: 0.8742523789405823\n",
            "Batch: 347, Loss: 0.9727157354354858\n",
            "Batch: 348, Loss: 1.0206319093704224\n",
            "Batch: 349, Loss: 0.9736632704734802\n",
            "Batch: 350, Loss: 0.8547430038452148\n",
            "Batch: 351, Loss: 0.9206289649009705\n",
            "Batch: 352, Loss: 0.8893576860427856\n",
            "Batch: 353, Loss: 0.8894689083099365\n",
            "Batch: 354, Loss: 0.8381764888763428\n",
            "Batch: 355, Loss: 0.9566295742988586\n",
            "Batch: 356, Loss: 0.975506603717804\n",
            "Batch: 357, Loss: 0.8092014789581299\n",
            "Batch: 358, Loss: 0.8831229209899902\n",
            "Batch: 359, Loss: 0.9761731028556824\n",
            "Batch: 360, Loss: 0.8527743816375732\n",
            "Batch: 361, Loss: 0.9204022884368896\n",
            "Batch: 362, Loss: 0.9223178625106812\n",
            "Batch: 363, Loss: 0.8850990533828735\n",
            "Batch: 364, Loss: 0.8838069438934326\n",
            "Batch: 365, Loss: 0.9067901968955994\n",
            "Batch: 366, Loss: 1.0579488277435303\n",
            "Batch: 367, Loss: 0.8031981587409973\n",
            "Batch: 368, Loss: 0.8904291391372681\n",
            "Batch: 369, Loss: 0.8885177969932556\n",
            "Batch: 370, Loss: 0.884740948677063\n",
            "Batch: 371, Loss: 0.94907146692276\n",
            "Batch: 372, Loss: 0.8310814499855042\n",
            "Batch: 373, Loss: 0.8137192130088806\n",
            "Batch: 374, Loss: 0.8131090998649597\n",
            "Batch: 375, Loss: 0.7660782933235168\n",
            "Batch: 376, Loss: 0.8643999695777893\n",
            "Batch: 377, Loss: 0.9422706961631775\n",
            "Batch: 378, Loss: 0.8855081796646118\n",
            "Batch: 379, Loss: 0.7673273682594299\n",
            "Batch: 380, Loss: 0.81331467628479\n",
            "Batch: 381, Loss: 0.9320032000541687\n",
            "Batch: 382, Loss: 0.8649585843086243\n",
            "Batch: 383, Loss: 0.9236421585083008\n",
            "Batch: 384, Loss: 0.7722771763801575\n",
            "Batch: 385, Loss: 0.8975469470024109\n",
            "Batch: 386, Loss: 0.8878238797187805\n",
            "Batch: 387, Loss: 0.5921803116798401\n",
            "Batch: 388, Loss: 0.8909934759140015\n",
            "Batch: 389, Loss: 0.894393801689148\n",
            "Batch: 390, Loss: 0.9825974702835083\n",
            "Batch: 391, Loss: 0.8629487156867981\n",
            "Batch: 392, Loss: 0.8722584247589111\n",
            "Batch: 393, Loss: 0.9258860945701599\n",
            "Batch: 394, Loss: 0.9161369800567627\n",
            "Batch: 395, Loss: 0.9599263072013855\n",
            "Batch: 396, Loss: 0.7576858401298523\n",
            "Batch: 397, Loss: 0.8301110863685608\n",
            "Batch: 398, Loss: 0.7295325994491577\n",
            "Batch: 399, Loss: 0.9730862379074097\n",
            "Batch: 400, Loss: 0.9307124614715576\n",
            "Batch: 401, Loss: 0.7890152931213379\n",
            "Batch: 402, Loss: 0.8173394203186035\n",
            "Batch: 403, Loss: 0.8856227397918701\n",
            "Batch: 404, Loss: 0.7846046686172485\n",
            "Batch: 405, Loss: 0.7618322968482971\n",
            "Batch: 406, Loss: 0.9257919192314148\n",
            "Batch: 407, Loss: 0.8842496275901794\n",
            "Batch: 408, Loss: 0.8112576007843018\n",
            "Batch: 409, Loss: 0.9705411791801453\n",
            "Batch: 410, Loss: 0.7815613150596619\n",
            "Batch: 411, Loss: 0.7933449745178223\n",
            "Batch: 412, Loss: 0.8094360828399658\n",
            "Batch: 413, Loss: 0.9492308497428894\n",
            "Batch: 414, Loss: 0.9741339087486267\n",
            "Batch: 415, Loss: 0.9205374717712402\n",
            "Batch: 416, Loss: 0.8660680055618286\n",
            "Batch: 417, Loss: 0.9039759635925293\n",
            "Batch: 418, Loss: 0.7970016598701477\n",
            "Batch: 419, Loss: 0.8234348297119141\n",
            "Batch: 420, Loss: 0.9064974784851074\n",
            "Batch: 421, Loss: 0.7526271343231201\n",
            "Batch: 422, Loss: 1.0503318309783936\n",
            "Batch: 423, Loss: 0.7594854831695557\n",
            "Batch: 424, Loss: 0.6812638640403748\n",
            "Batch: 425, Loss: 0.9555196166038513\n",
            "Batch: 426, Loss: 0.7376278042793274\n",
            "Batch: 427, Loss: 0.8431447744369507\n",
            "Batch: 428, Loss: 0.8815689086914062\n",
            "Batch: 429, Loss: 0.8358432650566101\n",
            "Batch: 430, Loss: 0.8098348379135132\n",
            "Batch: 431, Loss: 0.7581433653831482\n",
            "Batch: 432, Loss: 0.6801069974899292\n",
            "Batch: 433, Loss: 0.886668860912323\n",
            "Batch: 434, Loss: 0.7102667093276978\n",
            "Batch: 435, Loss: 0.8478506207466125\n",
            "Batch: 436, Loss: 0.8805925846099854\n",
            "Batch: 437, Loss: 0.9708711504936218\n",
            "Batch: 438, Loss: 0.8538365960121155\n",
            "Batch: 439, Loss: 0.8771015405654907\n",
            "Batch: 440, Loss: 0.8507150411605835\n",
            "Batch: 441, Loss: 0.8652411103248596\n",
            "Batch: 442, Loss: 0.7155758142471313\n",
            "Batch: 443, Loss: 0.8596690893173218\n",
            "Batch: 444, Loss: 0.724256694316864\n",
            "Batch: 445, Loss: 0.886315643787384\n",
            "Batch: 446, Loss: 0.7182594537734985\n",
            "Batch: 447, Loss: 0.8924360871315002\n",
            "Batch: 448, Loss: 0.8969933390617371\n",
            "Batch: 449, Loss: 0.8984818458557129\n",
            "Batch: 450, Loss: 0.8505533933639526\n",
            "Batch: 451, Loss: 0.963521420955658\n",
            "Batch: 452, Loss: 1.0614919662475586\n",
            "Batch: 453, Loss: 0.7783735990524292\n",
            "Batch: 454, Loss: 0.8335102796554565\n",
            "Batch: 455, Loss: 0.9947052597999573\n",
            "Batch: 456, Loss: 0.7210046052932739\n",
            "Batch: 457, Loss: 0.7364028692245483\n",
            "Batch: 458, Loss: 0.7309616208076477\n",
            "Batch: 459, Loss: 0.7162827253341675\n",
            "Batch: 460, Loss: 0.6828448176383972\n",
            "Batch: 461, Loss: 0.7996336221694946\n",
            "Batch: 462, Loss: 0.8469101190567017\n",
            "Batch: 463, Loss: 1.019700288772583\n",
            "Batch: 464, Loss: 0.81292724609375\n",
            "Batch: 465, Loss: 0.8566833138465881\n",
            "Batch: 466, Loss: 0.7959211468696594\n",
            "Batch: 467, Loss: 0.8799582719802856\n",
            "Batch: 468, Loss: 0.8677557110786438\n",
            "Batch: 469, Loss: 0.8151605725288391\n",
            "Batch: 470, Loss: 0.7727619409561157\n",
            "Batch: 471, Loss: 0.7839208245277405\n",
            "Batch: 472, Loss: 0.7476712465286255\n",
            "Batch: 473, Loss: 0.8931441903114319\n",
            "Batch: 474, Loss: 0.7824522256851196\n",
            "Batch: 475, Loss: 0.7838314175605774\n",
            "Batch: 476, Loss: 0.733286440372467\n",
            "Batch: 477, Loss: 0.7414096593856812\n",
            "Batch: 478, Loss: 0.7301003336906433\n",
            "Batch: 479, Loss: 0.8647900819778442\n",
            "Batch: 480, Loss: 0.7664608359336853\n",
            "Batch: 481, Loss: 0.6764508485794067\n",
            "Batch: 482, Loss: 0.8266417980194092\n",
            "Batch: 483, Loss: 0.8153027296066284\n",
            "Batch: 484, Loss: 0.8251276016235352\n",
            "Batch: 485, Loss: 0.6247997283935547\n",
            "Batch: 486, Loss: 0.7449226975440979\n",
            "Batch: 487, Loss: 0.7727482914924622\n",
            "Batch: 488, Loss: 0.6419260501861572\n",
            "Batch: 489, Loss: 0.8319789171218872\n",
            "Batch: 490, Loss: 0.7450087070465088\n",
            "Batch: 491, Loss: 0.5867926478385925\n",
            "Batch: 492, Loss: 0.6815365552902222\n",
            "Batch: 493, Loss: 0.7501040697097778\n",
            "Batch: 494, Loss: 0.6335069537162781\n",
            "Batch: 495, Loss: 0.9039781093597412\n",
            "Batch: 496, Loss: 0.9407378435134888\n",
            "Batch: 497, Loss: 0.7830535769462585\n",
            "Batch: 498, Loss: 0.7702823281288147\n",
            "Batch: 499, Loss: 0.7954212427139282\n",
            "Batch: 500, Loss: 0.828656792640686\n",
            "Batch: 501, Loss: 0.6875040531158447\n",
            "Batch: 502, Loss: 0.672936201095581\n",
            "Batch: 503, Loss: 0.737656831741333\n",
            "Batch: 504, Loss: 0.7883365750312805\n",
            "Batch: 505, Loss: 0.8771887421607971\n",
            "Batch: 506, Loss: 0.7339561581611633\n",
            "Batch: 507, Loss: 0.6748456358909607\n",
            "Batch: 508, Loss: 0.9704158306121826\n",
            "Batch: 509, Loss: 0.7333959341049194\n",
            "Batch: 510, Loss: 0.8187532424926758\n",
            "Batch: 511, Loss: 0.8454983234405518\n",
            "Batch: 512, Loss: 0.7328206300735474\n",
            "Batch: 513, Loss: 0.9156755805015564\n",
            "Batch: 514, Loss: 0.7030952572822571\n",
            "Batch: 515, Loss: 0.9119809865951538\n",
            "Batch: 516, Loss: 0.8237977623939514\n",
            "Batch: 517, Loss: 0.8064717054367065\n",
            "Batch: 518, Loss: 0.7936767339706421\n",
            "Batch: 519, Loss: 0.6412560343742371\n",
            "Batch: 520, Loss: 0.7320554852485657\n",
            "Batch: 521, Loss: 0.5816064476966858\n",
            "Batch: 522, Loss: 0.7681926488876343\n",
            "Batch: 523, Loss: 0.8619713187217712\n",
            "Batch: 524, Loss: 0.8951335549354553\n",
            "Batch: 525, Loss: 0.8034394383430481\n",
            "Batch: 526, Loss: 0.6608410477638245\n",
            "Batch: 527, Loss: 0.7896682620048523\n",
            "Batch: 528, Loss: 0.5994271636009216\n",
            "Batch: 529, Loss: 0.7104863524436951\n",
            "Batch: 530, Loss: 0.8121790289878845\n",
            "Batch: 531, Loss: 0.8122904300689697\n",
            "Batch: 532, Loss: 0.6990258097648621\n",
            "Batch: 533, Loss: 0.648271918296814\n",
            "Batch: 534, Loss: 0.6982792019844055\n",
            "Batch: 535, Loss: 0.6977294087409973\n",
            "Batch: 536, Loss: 0.6070196032524109\n",
            "Batch: 537, Loss: 0.7699664235115051\n",
            "Batch: 538, Loss: 0.7679359316825867\n",
            "Batch: 539, Loss: 0.5719768404960632\n",
            "Batch: 540, Loss: 0.6565213799476624\n",
            "Batch: 541, Loss: 0.6275808215141296\n",
            "Batch: 542, Loss: 0.6773708462715149\n",
            "Batch: 543, Loss: 0.7926729917526245\n",
            "Batch: 544, Loss: 0.7022356986999512\n",
            "Batch: 545, Loss: 0.8554904460906982\n",
            "Batch: 546, Loss: 0.8044986724853516\n",
            "Batch: 547, Loss: 0.8353396058082581\n",
            "Batch: 548, Loss: 0.7810187339782715\n",
            "Batch: 549, Loss: 0.7163552045822144\n",
            "Batch: 550, Loss: 0.6067243814468384\n",
            "Batch: 551, Loss: 0.814659059047699\n",
            "Batch: 552, Loss: 0.6828365325927734\n",
            "Batch: 553, Loss: 0.6910256147384644\n",
            "Batch: 554, Loss: 0.6246902942657471\n",
            "Batch: 555, Loss: 0.6801997423171997\n",
            "Batch: 556, Loss: 0.5987306237220764\n",
            "Batch: 557, Loss: 0.6237297654151917\n",
            "Batch: 558, Loss: 0.8075141906738281\n",
            "Batch: 559, Loss: 0.7598639726638794\n",
            "Batch: 560, Loss: 0.7310505509376526\n",
            "Batch: 561, Loss: 0.6259545087814331\n",
            "Batch: 562, Loss: 0.5659352540969849\n",
            "Batch: 563, Loss: 0.8234436511993408\n",
            "Batch: 564, Loss: 0.71482253074646\n",
            "Batch: 565, Loss: 0.7079187035560608\n",
            "Batch: 566, Loss: 0.7056533098220825\n",
            "Batch: 567, Loss: 0.7099377512931824\n",
            "Batch: 568, Loss: 0.6773277521133423\n",
            "Batch: 569, Loss: 0.7676053047180176\n",
            "Batch: 570, Loss: 0.6196939945220947\n",
            "Batch: 571, Loss: 0.7861593961715698\n",
            "Batch: 572, Loss: 0.6208330988883972\n",
            "Batch: 573, Loss: 0.7778885364532471\n",
            "Batch: 574, Loss: 0.6763424873352051\n",
            "Batch: 575, Loss: 0.7680953741073608\n",
            "Batch: 576, Loss: 0.7287395000457764\n",
            "Batch: 577, Loss: 0.6801403760910034\n",
            "Batch: 578, Loss: 0.7436361312866211\n",
            "Batch: 579, Loss: 0.7413076758384705\n",
            "Batch: 580, Loss: 0.6562256217002869\n",
            "Batch: 581, Loss: 0.8694367408752441\n",
            "Batch: 582, Loss: 0.5909165143966675\n",
            "Batch: 583, Loss: 0.6813505291938782\n",
            "Batch: 584, Loss: 0.7465162873268127\n",
            "Batch: 585, Loss: 0.6455032229423523\n",
            "Batch: 586, Loss: 0.6227275729179382\n",
            "Batch: 587, Loss: 0.6165330410003662\n",
            "Batch: 588, Loss: 0.8517565131187439\n",
            "Batch: 589, Loss: 0.7406729459762573\n",
            "Batch: 590, Loss: 0.6439403891563416\n",
            "Batch: 591, Loss: 0.832514226436615\n",
            "Batch: 592, Loss: 0.6787759065628052\n",
            "Batch: 593, Loss: 0.7139796614646912\n",
            "Batch: 594, Loss: 0.793912410736084\n",
            "Batch: 595, Loss: 1.082614779472351\n",
            "Batch: 596, Loss: 0.8261108994483948\n",
            "Batch: 597, Loss: 0.6746484637260437\n",
            "Batch: 598, Loss: 0.7994933724403381\n",
            "Batch: 599, Loss: 0.7539932131767273\n",
            "Batch: 600, Loss: 0.680988609790802\n",
            "Batch: 601, Loss: 0.5973629951477051\n",
            "Batch: 602, Loss: 0.6500413417816162\n",
            "Batch: 603, Loss: 0.8530051112174988\n",
            "Batch: 604, Loss: 0.7372016310691833\n",
            "Batch: 605, Loss: 0.7204374074935913\n",
            "Batch: 606, Loss: 0.9100418090820312\n",
            "Batch: 607, Loss: 0.5812629461288452\n",
            "Batch: 608, Loss: 0.7281333208084106\n",
            "Batch: 609, Loss: 0.9157388806343079\n",
            "Batch: 610, Loss: 0.7491305470466614\n",
            "Batch: 611, Loss: 0.6063945889472961\n",
            "Batch: 612, Loss: 0.6295720934867859\n",
            "Batch: 613, Loss: 0.5826237201690674\n",
            "Batch: 614, Loss: 0.9954559803009033\n",
            "Batch: 615, Loss: 0.7363439202308655\n",
            "Batch: 616, Loss: 0.6387534737586975\n",
            "Batch: 617, Loss: 0.715178906917572\n",
            "Batch: 618, Loss: 0.6519582271575928\n",
            "Batch: 619, Loss: 0.6059600114822388\n",
            "Batch: 620, Loss: 0.8239446878433228\n",
            "Batch: 621, Loss: 0.7041512727737427\n",
            "Batch: 622, Loss: 0.7517615556716919\n",
            "Batch: 623, Loss: 0.7834028601646423\n",
            "Batch: 624, Loss: 0.6568654179573059\n",
            "Batch: 625, Loss: 0.8303291201591492\n",
            "Batch: 626, Loss: 0.5953484177589417\n",
            "Batch: 627, Loss: 0.7506959438323975\n",
            "Batch: 628, Loss: 0.7589165568351746\n",
            "Batch: 629, Loss: 0.8066144585609436\n",
            "Batch: 630, Loss: 0.7289687991142273\n",
            "Batch: 631, Loss: 0.7028437256813049\n",
            "Batch: 632, Loss: 0.5818382501602173\n",
            "Batch: 633, Loss: 0.7136284708976746\n",
            "Batch: 634, Loss: 0.6642637848854065\n",
            "Batch: 635, Loss: 0.660400390625\n",
            "Batch: 636, Loss: 0.6658994555473328\n",
            "Batch: 637, Loss: 0.7241387963294983\n",
            "Batch: 638, Loss: 0.742098331451416\n",
            "Batch: 639, Loss: 0.8062554597854614\n",
            "Batch: 640, Loss: 0.7051828503608704\n",
            "Batch: 641, Loss: 0.5880023241043091\n",
            "Batch: 642, Loss: 0.5972520709037781\n",
            "Batch: 643, Loss: 0.7716394066810608\n",
            "Batch: 644, Loss: 0.7747750878334045\n",
            "Batch: 645, Loss: 0.5889202952384949\n",
            "Batch: 646, Loss: 0.7427261471748352\n",
            "Batch: 647, Loss: 0.7788755297660828\n",
            "Batch: 648, Loss: 0.7138064503669739\n",
            "Batch: 649, Loss: 0.7940832376480103\n",
            "Batch: 650, Loss: 0.6026303172111511\n",
            "Batch: 651, Loss: 0.6958103179931641\n",
            "Batch: 652, Loss: 0.5492735505104065\n",
            "Batch: 653, Loss: 0.789811372756958\n",
            "Batch: 654, Loss: 0.4946357011795044\n",
            "Batch: 655, Loss: 0.5723232626914978\n",
            "Batch: 656, Loss: 0.6144699454307556\n",
            "Batch: 657, Loss: 0.5623006820678711\n",
            "Batch: 658, Loss: 0.6601786613464355\n",
            "Batch: 659, Loss: 0.6564254760742188\n",
            "Batch: 660, Loss: 0.6380404829978943\n",
            "Batch: 661, Loss: 0.7640619874000549\n",
            "Batch: 662, Loss: 0.5447457432746887\n",
            "Batch: 663, Loss: 0.8048111796379089\n",
            "Batch: 664, Loss: 0.7843190431594849\n",
            "Batch: 665, Loss: 0.6503065228462219\n",
            "Batch: 666, Loss: 0.7309139966964722\n",
            "Batch: 667, Loss: 0.6409375667572021\n",
            "Batch: 668, Loss: 0.7250080704689026\n",
            "Batch: 669, Loss: 0.6777895092964172\n",
            "Batch: 670, Loss: 0.6231792569160461\n",
            "Batch: 671, Loss: 0.5729854106903076\n",
            "Batch: 672, Loss: 0.8173567652702332\n",
            "Batch: 673, Loss: 0.6930330991744995\n",
            "Batch: 674, Loss: 0.7172976732254028\n",
            "Batch: 675, Loss: 0.9463987946510315\n",
            "Batch: 676, Loss: 0.7621044516563416\n",
            "Batch: 677, Loss: 0.7005479335784912\n",
            "Batch: 678, Loss: 0.7305524945259094\n",
            "Batch: 679, Loss: 0.6690582036972046\n",
            "Batch: 680, Loss: 0.8534799814224243\n",
            "Batch: 681, Loss: 0.6198570728302002\n",
            "Batch: 682, Loss: 0.5953577756881714\n",
            "Batch: 683, Loss: 0.6397839188575745\n",
            "Batch: 684, Loss: 0.6284045577049255\n",
            "Batch: 685, Loss: 0.6421740651130676\n",
            "Batch: 686, Loss: 0.7030202150344849\n",
            "Batch: 687, Loss: 0.8566727638244629\n",
            "Batch: 688, Loss: 0.7487527132034302\n",
            "Batch: 689, Loss: 0.6380390524864197\n",
            "Batch: 690, Loss: 0.7255565524101257\n",
            "Batch: 691, Loss: 0.7232861518859863\n",
            "Batch: 692, Loss: 0.7224075198173523\n",
            "Batch: 693, Loss: 0.763626754283905\n",
            "Batch: 694, Loss: 0.7034002542495728\n",
            "Batch: 695, Loss: 0.7502437233924866\n",
            "Batch: 696, Loss: 0.850501298904419\n",
            "Batch: 697, Loss: 0.6978357434272766\n",
            "Batch: 698, Loss: 0.7250255346298218\n",
            "Batch: 699, Loss: 0.8582485318183899\n",
            "Batch: 700, Loss: 0.8512310981750488\n",
            "Batch: 701, Loss: 0.6327058672904968\n",
            "Batch: 702, Loss: 0.606533408164978\n",
            "Batch: 703, Loss: 0.6422128081321716\n",
            "Batch: 704, Loss: 0.6038917899131775\n",
            "Batch: 705, Loss: 0.6565703749656677\n",
            "Batch: 706, Loss: 0.6937651634216309\n",
            "Batch: 707, Loss: 0.6600796580314636\n",
            "Batch: 708, Loss: 0.7112306356430054\n",
            "Batch: 709, Loss: 0.7098637819290161\n",
            "Batch: 710, Loss: 0.5268063545227051\n",
            "Batch: 711, Loss: 0.6000341773033142\n",
            "Batch: 712, Loss: 0.5753586888313293\n",
            "Batch: 713, Loss: 0.6871337890625\n",
            "Batch: 714, Loss: 0.8461329936981201\n",
            "Batch: 715, Loss: 0.845124363899231\n",
            "Batch: 716, Loss: 0.5139801502227783\n",
            "Batch: 717, Loss: 0.6774196624755859\n",
            "Batch: 718, Loss: 0.8603082895278931\n",
            "Batch: 719, Loss: 0.6650300621986389\n",
            "Batch: 720, Loss: 0.5893146395683289\n",
            "Batch: 721, Loss: 0.5107768774032593\n",
            "Batch: 722, Loss: 0.7209864854812622\n",
            "Batch: 723, Loss: 0.7577978372573853\n",
            "Batch: 724, Loss: 0.5715569853782654\n",
            "Batch: 725, Loss: 0.6343383193016052\n",
            "Batch: 726, Loss: 0.7381986975669861\n",
            "Batch: 727, Loss: 0.637920081615448\n",
            "Batch: 728, Loss: 0.46756383776664734\n",
            "Batch: 729, Loss: 0.5611651539802551\n",
            "Batch: 730, Loss: 0.7167226672172546\n",
            "Batch: 731, Loss: 0.563602864742279\n",
            "Batch: 732, Loss: 0.5867384672164917\n",
            "Batch: 733, Loss: 0.5885021686553955\n",
            "Batch: 734, Loss: 0.6150480508804321\n",
            "Batch: 735, Loss: 0.7212916016578674\n",
            "Batch: 736, Loss: 0.43099039793014526\n",
            "Batch: 737, Loss: 0.6484168767929077\n",
            "Batch: 738, Loss: 0.6372349262237549\n",
            "Batch: 739, Loss: 0.6757636666297913\n",
            "Batch: 740, Loss: 0.7488604187965393\n",
            "Batch: 741, Loss: 0.6898055672645569\n",
            "Batch: 742, Loss: 0.6025683283805847\n",
            "Batch: 743, Loss: 0.6624895930290222\n",
            "Batch: 744, Loss: 0.6828665137290955\n",
            "Batch: 745, Loss: 0.5264500975608826\n",
            "Batch: 746, Loss: 0.791111409664154\n",
            "Batch: 747, Loss: 0.5684612989425659\n",
            "Batch: 748, Loss: 0.762029230594635\n",
            "Batch: 749, Loss: 0.6306822299957275\n",
            "Batch: 750, Loss: 0.7075437307357788\n",
            "Batch: 751, Loss: 0.7740753889083862\n",
            "Batch: 752, Loss: 0.5576768517494202\n",
            "Batch: 753, Loss: 0.5311468243598938\n",
            "Batch: 754, Loss: 0.6219112277030945\n",
            "Batch: 755, Loss: 0.8577664494514465\n",
            "Batch: 756, Loss: 0.9662827849388123\n",
            "Batch: 757, Loss: 0.6414340734481812\n",
            "Batch: 758, Loss: 0.679150402545929\n",
            "Batch: 759, Loss: 0.6280474662780762\n",
            "Batch: 760, Loss: 0.6839976906776428\n",
            "Batch: 761, Loss: 0.7859910726547241\n",
            "Batch: 762, Loss: 0.571248471736908\n",
            "Batch: 763, Loss: 0.5666601061820984\n",
            "Batch: 764, Loss: 0.7858747243881226\n",
            "Batch: 765, Loss: 0.6183181405067444\n",
            "Batch: 766, Loss: 0.653985321521759\n",
            "Batch: 767, Loss: 0.5407640933990479\n",
            "Batch: 768, Loss: 0.5331249833106995\n",
            "Batch: 769, Loss: 0.6551913619041443\n",
            "Batch: 770, Loss: 0.7182613015174866\n",
            "Batch: 771, Loss: 0.6026711463928223\n",
            "Batch: 772, Loss: 0.6185632348060608\n",
            "Batch: 773, Loss: 0.6852664947509766\n",
            "Batch: 774, Loss: 0.7799773812294006\n",
            "Batch: 775, Loss: 0.7940467596054077\n",
            "Batch: 776, Loss: 0.5412526726722717\n",
            "Batch: 777, Loss: 0.6224319934844971\n",
            "Batch: 778, Loss: 0.6344006061553955\n",
            "Batch: 779, Loss: 0.7858192920684814\n",
            "Batch: 780, Loss: 0.5849111676216125\n",
            "Batch: 781, Loss: 0.6281523108482361\n",
            "Batch: 782, Loss: 0.654203474521637\n",
            "Batch: 783, Loss: 0.4942704737186432\n",
            "Batch: 784, Loss: 0.6027223467826843\n",
            "Batch: 785, Loss: 0.43808799982070923\n",
            "Batch: 786, Loss: 0.7119549512863159\n",
            "Batch: 787, Loss: 0.6328842639923096\n",
            "Batch: 788, Loss: 0.7231288552284241\n",
            "Batch: 789, Loss: 0.5687597393989563\n",
            "Batch: 790, Loss: 0.6730513572692871\n",
            "Batch: 791, Loss: 0.7408499121665955\n",
            "Batch: 792, Loss: 0.6557093262672424\n",
            "Batch: 793, Loss: 0.5620623230934143\n",
            "Batch: 794, Loss: 0.6990829110145569\n",
            "Batch: 795, Loss: 0.7397968769073486\n",
            "Batch: 796, Loss: 0.7857415676116943\n",
            "Batch: 797, Loss: 0.45920801162719727\n",
            "Batch: 798, Loss: 0.796527624130249\n",
            "Batch: 799, Loss: 0.8505105376243591\n",
            "Batch: 800, Loss: 0.6444640755653381\n",
            "Batch: 801, Loss: 0.7757431864738464\n",
            "Batch: 802, Loss: 0.6063606142997742\n",
            "Batch: 803, Loss: 0.5656057000160217\n",
            "Batch: 804, Loss: 0.5907294154167175\n",
            "Batch: 805, Loss: 0.8203865885734558\n",
            "Batch: 806, Loss: 0.753963053226471\n",
            "Batch: 807, Loss: 0.716637909412384\n",
            "Batch: 808, Loss: 0.7379147410392761\n",
            "Batch: 809, Loss: 0.6200859546661377\n",
            "Batch: 810, Loss: 0.6521020531654358\n",
            "Batch: 811, Loss: 0.645011842250824\n",
            "Batch: 812, Loss: 0.7428694367408752\n",
            "Batch: 813, Loss: 0.8248671889305115\n",
            "Batch: 814, Loss: 0.5601865649223328\n",
            "Batch: 815, Loss: 0.6150840520858765\n",
            "Batch: 816, Loss: 0.6041164994239807\n",
            "Batch: 817, Loss: 0.48815613985061646\n",
            "Batch: 818, Loss: 0.6281843185424805\n",
            "Batch: 819, Loss: 0.4949638247489929\n",
            "Batch: 820, Loss: 0.6521658897399902\n",
            "Batch: 821, Loss: 0.6419199705123901\n",
            "Batch: 822, Loss: 0.5988394618034363\n",
            "Batch: 823, Loss: 0.8148850202560425\n",
            "Batch: 824, Loss: 0.6400477886199951\n",
            "Batch: 825, Loss: 0.7517824769020081\n",
            "Batch: 826, Loss: 0.5084084868431091\n",
            "Batch: 827, Loss: 0.7202269434928894\n",
            "Batch: 828, Loss: 0.5171689987182617\n",
            "Batch: 829, Loss: 0.8151130080223083\n",
            "Batch: 830, Loss: 0.7826482057571411\n",
            "Batch: 831, Loss: 0.6649870872497559\n",
            "Batch: 832, Loss: 0.5785417556762695\n",
            "Batch: 833, Loss: 0.5158247947692871\n",
            "Batch: 834, Loss: 0.5649315118789673\n",
            "Batch: 835, Loss: 0.7199298143386841\n",
            "Batch: 836, Loss: 0.6904112100601196\n",
            "Batch: 837, Loss: 0.5897151231765747\n",
            "Batch: 838, Loss: 0.550326943397522\n",
            "Batch: 839, Loss: 0.5124407410621643\n",
            "Batch: 840, Loss: 0.604079008102417\n",
            "Batch: 841, Loss: 0.7670825123786926\n",
            "Batch: 842, Loss: 0.7694797515869141\n",
            "Batch: 843, Loss: 0.4963315725326538\n",
            "Batch: 844, Loss: 0.5648619532585144\n",
            "Batch: 845, Loss: 0.5140897035598755\n",
            "Batch: 846, Loss: 0.5417199730873108\n",
            "Batch: 847, Loss: 0.9007357954978943\n",
            "Batch: 848, Loss: 0.9658144116401672\n",
            "Batch: 849, Loss: 0.6200382709503174\n",
            "Batch: 850, Loss: 0.5528854131698608\n",
            "Batch: 851, Loss: 0.7188135385513306\n",
            "Batch: 852, Loss: 0.797662615776062\n",
            "Batch: 853, Loss: 0.6577242612838745\n",
            "Batch: 854, Loss: 0.5464009642601013\n",
            "Batch: 855, Loss: 0.5963256359100342\n",
            "Batch: 856, Loss: 0.5517163276672363\n",
            "Batch: 857, Loss: 0.5644497275352478\n",
            "Batch: 858, Loss: 0.4729515016078949\n",
            "Batch: 859, Loss: 0.5971854329109192\n",
            "Batch: 860, Loss: 0.6971406936645508\n",
            "Batch: 861, Loss: 0.5674610733985901\n",
            "Batch: 862, Loss: 0.6881705522537231\n",
            "Batch: 863, Loss: 0.5960100293159485\n",
            "Batch: 864, Loss: 0.571345329284668\n",
            "Batch: 865, Loss: 0.8028832077980042\n",
            "Batch: 866, Loss: 0.7214369773864746\n",
            "Batch: 867, Loss: 0.6991302371025085\n",
            "Batch: 868, Loss: 0.6198412179946899\n",
            "Batch: 869, Loss: 0.45495328307151794\n",
            "Batch: 870, Loss: 0.6951145529747009\n",
            "Batch: 871, Loss: 0.7801210284233093\n",
            "Batch: 872, Loss: 0.6003425121307373\n",
            "Batch: 873, Loss: 0.7451518774032593\n",
            "Batch: 874, Loss: 1.0854837894439697\n",
            "Batch: 875, Loss: 0.8186969757080078\n",
            "Batch: 876, Loss: 0.694145679473877\n",
            "Batch: 877, Loss: 0.6241816878318787\n",
            "Batch: 878, Loss: 0.7337243556976318\n",
            "Batch: 879, Loss: 0.6854197978973389\n",
            "Batch: 880, Loss: 0.6761912107467651\n",
            "Batch: 881, Loss: 0.4673535227775574\n",
            "Batch: 882, Loss: 0.6528578996658325\n",
            "Batch: 883, Loss: 0.502355694770813\n",
            "Batch: 884, Loss: 0.6580905914306641\n",
            "Batch: 885, Loss: 0.7264502644538879\n",
            "Batch: 886, Loss: 0.778481662273407\n",
            "Batch: 887, Loss: 0.48232463002204895\n",
            "Batch: 888, Loss: 0.5711605548858643\n",
            "Batch: 889, Loss: 0.6398914456367493\n",
            "Batch: 890, Loss: 0.4970901310443878\n",
            "Batch: 891, Loss: 0.5883349180221558\n",
            "Batch: 892, Loss: 0.4844816327095032\n",
            "Batch: 893, Loss: 0.8091631531715393\n",
            "Batch: 894, Loss: 0.5772485733032227\n",
            "Batch: 895, Loss: 0.6824581027030945\n",
            "Batch: 896, Loss: 0.9438701868057251\n",
            "Batch: 897, Loss: 0.5400370359420776\n",
            "Batch: 898, Loss: 0.7408401966094971\n",
            "Batch: 899, Loss: 0.5619274377822876\n",
            "Batch: 900, Loss: 0.4685729146003723\n",
            "Batch: 901, Loss: 0.586601734161377\n",
            "Batch: 902, Loss: 0.5675721764564514\n",
            "Batch: 903, Loss: 0.6790302395820618\n",
            "Batch: 904, Loss: 0.5487197041511536\n",
            "Batch: 905, Loss: 0.7346677184104919\n",
            "Batch: 906, Loss: 0.60404372215271\n",
            "Batch: 907, Loss: 0.6889986395835876\n",
            "Batch: 908, Loss: 0.7185705900192261\n",
            "Batch: 909, Loss: 0.4906315505504608\n",
            "Batch: 910, Loss: 0.4848899841308594\n",
            "Batch: 911, Loss: 0.6424283981323242\n",
            "Batch: 912, Loss: 0.5369298458099365\n",
            "Batch: 913, Loss: 0.46389785408973694\n",
            "Batch: 914, Loss: 0.620110809803009\n",
            "Batch: 915, Loss: 0.7075643539428711\n",
            "Batch: 916, Loss: 0.644456684589386\n",
            "Batch: 917, Loss: 0.7690984010696411\n",
            "Batch: 918, Loss: 0.6699065566062927\n",
            "Batch: 919, Loss: 0.8278055787086487\n",
            "Batch: 920, Loss: 0.7285965085029602\n",
            "Batch: 921, Loss: 0.5827052593231201\n",
            "Batch: 922, Loss: 0.5327875018119812\n",
            "Batch: 923, Loss: 0.628761887550354\n",
            "Batch: 924, Loss: 0.5486899018287659\n",
            "Batch: 925, Loss: 0.7654621005058289\n",
            "Batch: 926, Loss: 0.5996806621551514\n",
            "Batch: 927, Loss: 0.6182435154914856\n",
            "Batch: 928, Loss: 0.692462682723999\n",
            "Batch: 929, Loss: 0.4430762827396393\n",
            "Batch: 930, Loss: 0.7466270923614502\n",
            "Batch: 931, Loss: 0.5913406610488892\n",
            "Batch: 932, Loss: 0.5907335877418518\n",
            "Batch: 933, Loss: 0.646734893321991\n",
            "Batch: 934, Loss: 0.5607890486717224\n",
            "Batch: 935, Loss: 0.49238529801368713\n",
            "Batch: 936, Loss: 0.6768471598625183\n",
            "Batch: 937, Loss: 0.464010089635849\n",
            "Batch: 938, Loss: 0.5052196979522705\n",
            "Training loss: 1.030691686819103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OVDFUnzFGpr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b9656e-f877-4e34-d826-2ad2b9cccc93"
      },
      "source": [
        "60000/64 # batches"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "937.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWf1SWuiFGmn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ_QUMXLFGjr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}